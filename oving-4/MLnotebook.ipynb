{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class LongShortTermMemoryModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emoji_encoding, encoding_size):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(encoding_size, 128, batch_first=True)  # 128 is the state size\n",
    "        self.dense = nn.Linear(128, emoji_encoding)  # 128 is the state size\n",
    "\n",
    "    def reset(self, batch_size):  # Reset states prior to new input sequence\n",
    "        # takes in a batch_size so that the hidden_state, and cell_state is the correct size\n",
    "        # when operating on different batch_sizes\n",
    "        zero_state = torch.zeros(1, batch_size, 128)  # Shape: (batch_length, sequence_length, state size) Due to batch_first=True\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(x, (self.hidden_state, self.cell_state))\n",
    "        # lstm computes an output for each element in the seqeunce\n",
    "        # this means that out is of the shape (batch_size, sequence_length, state_size)\n",
    "        # ':' means take everything from this dimension\n",
    "        # '-1' means take the last element form this dimension\n",
    "        out = out[:, -1, :]\n",
    "        return self.dense(out)\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n",
    "\n",
    "    \n",
    "index_to_char = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', \n",
    "                 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n",
    "                 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
    "\n",
    "index_to_emoji = ['\\U0001F408', '\\U0001F400', '\\U0001F3A9']\n",
    "\n",
    "encoding_size = len(index_to_char)\n",
    "emoji_size    = len(index_to_emoji) \n",
    "\n",
    "char_encodings  = torch.eye(encoding_size).numpy()\n",
    "emoji_encodings = torch.eye(emoji_size).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 27])\n",
      "torch.Size([3, 3, 27])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "Batch_Size, Sequence_Length, Encoding_size: torch.Size([3, 3, 27])\n",
      "Batch_Size, Emoji_Encoding_Size: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.tensor([\n",
    "    [char_encodings[index_to_char.index('c')], char_encodings[index_to_char.index('a')], char_encodings[index_to_char.index('t')]],\n",
    "    [char_encodings[index_to_char.index('r')], char_encodings[index_to_char.index('a')], char_encodings[index_to_char.index('t')]],\n",
    "    [char_encodings[index_to_char.index('h')], char_encodings[index_to_char.index('a')], char_encodings[index_to_char.index('t')]]\n",
    "])\n",
    "    \n",
    "y_train = torch.tensor([emoji_encodings[0], emoji_encodings[1], emoji_encodings[2]])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Batch_Size, Sequence_Length, Encoding_size:\", x_train.shape)\n",
    "print(\"Batch_Size, Emoji_Encoding_Size:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongShortTermMemoryModel(emoji_size, encoding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n",
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n",
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n",
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n",
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n",
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n",
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n",
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n",
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n",
      "Cat: ğŸˆ\n",
      "Rat: ğŸ€\n",
      "Hat: ğŸ©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/xx_y_tyx4gbdt49zcgjb110m0000gn/T/ipykernel_75957/2379312386.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = model.f(torch.tensor(x_train[0].reshape(1, 3, encoding_size)))\n",
      "/var/folders/t3/xx_y_tyx4gbdt49zcgjb110m0000gn/T/ipykernel_75957/2379312386.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = model.f(torch.tensor(x_train[1].reshape(1, 3, encoding_size)))\n",
      "/var/folders/t3/xx_y_tyx4gbdt49zcgjb110m0000gn/T/ipykernel_75957/2379312386.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = model.f(torch.tensor(x_train[2].reshape(1, 3, encoding_size)))\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), 0.001)\n",
    "for epoch in range(100):\n",
    "    model.reset(3)\n",
    "    model.loss(x_train, y_train).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        # Generates emojis from the words \"cat\", \"rat\", and \"hat\"\n",
    "        model.reset(1)\n",
    "        y = model.f(torch.tensor(x_train[0].reshape(1, 3, encoding_size)))\n",
    "        print(\"Cat:\", index_to_emoji[y.argmax(1)])\n",
    "\n",
    "        model.reset(1)\n",
    "        y = model.f(torch.tensor(x_train[1].reshape(1, 3, encoding_size)))\n",
    "        print(\"Rat:\", index_to_emoji[y.argmax(1)])\n",
    "\n",
    "        model.reset(1)\n",
    "        y = model.f(torch.tensor(x_train[2].reshape(1, 3, encoding_size)))\n",
    "        print(\"Hat:\", index_to_emoji[y.argmax(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/xx_y_tyx4gbdt49zcgjb110m0000gn/T/ipykernel_75957/1545163512.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = model.f(torch.tensor(rt))\n"
     ]
    }
   ],
   "source": [
    "rt = torch.tensor([[char_encodings[index_to_char.index('h')], char_encodings[index_to_char.index('t')], char_encodings[index_to_char.index(' ')]]])\n",
    "#rt = rt.reshape(1, 3, encoding_size)\n",
    "\n",
    "model.reset(1)\n",
    "y = model.f(torch.tensor(rt))\n",
    "print(index_to_emoji[y.argmax(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([torch.eye(9).numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([char_encodings[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
